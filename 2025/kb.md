## 2024-01-28-Tue
1. DeepSeek Notes.   
    **DeepSeekMoE** (Mixture of Experts): Instead of activating the entire model for every task, DeepSeekMoE used a network of specialized and general-purpose “experts,” activating only the relevant ones. This innovation reduced computational waste and improved performance.  

    1. https://www.linkedin.com/pulse/deepseek-dummies-heres-what-you-need-know-ais-latest-schiessl-rdaue#:~:text=two%20groundbreaking%20technologies%3A-,deepseekmoe%20,-(Mixture%20of%20Experts)


    **DeepSeekMLA** (Multi-head Latent Attention): A solution to the heavy memory demands of inference. By compressing key-value storage, DeepSeek reduced memory usage significantly, making large-scale inference more efficient.   

    1. https://www.linkedin.com/pulse/deepseek-dummies-heres-what-you-need-know-ais-latest-schiessl-rdaue#:~:text=multi-head%20latent%20attention

1. Models are teaching themselves and other models, accelerating advancements and potentially paving the way toward AGI.

    https://www.linkedin.com/pulse/deepseek-dummies-heres-what-you-need-know-ais-latest-schiessl-rdaue#:~:text=models%20are%20teaching%20themselves

    1. sub topic 
1. second topic